{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "frequent-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "premier-footwear",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../logfiles/baseline/\n",
      "../logfiles/experiments/re_pruning/\n",
      "../logfiles/experiments/gd_top_k_mc_ac_dk/\n",
      "../logfiles/ablation_study/alexnet_mixed/\n",
      "../logfiles/ablation_study/resnet_mixed/\n",
      "../logfiles/ablation_study/admm_intra/\n",
      "../logfiles/ablation_study/admm_retrain/\n",
      "../logfiles/ablation_study/gd_top_k/\n",
      "../logfiles/ablation_study/gd_top_k_mc/\n",
      "../logfiles/ablation_study/gd_top_k_mc_ac/\n",
      "../logfiles/ablation_study/gd_top_k_mc_ac_dk/\n",
      "../logfiles/ablation_study/gd_top_k_mc_ac_dk_admm_intra/\n",
      "../logfiles/ablation_study/gd_top_k_mc_ac_dk_admm_retrain/\n",
      "../logfiles/ablation_study/re_pruning/\n",
      "../logfiles/ablation_study/re_pruning_admm_retrain/\n",
      "../logfiles/ablation_study/re_pruning_admm_intra/\n",
      "../logfiles/ablation_study/re_pruning_ac/\n",
      "../logfiles/ablation_study/re_pruning_ac_admm_intra/\n",
      "../logfiles/ablation_study/re_pruning_ac_admm_retrain/\n",
      "../logfiles/ablation_study/re_pruning_gd_top_k_mc_ac_dk_admm_intra/\n",
      "../logfiles/ablation_study/re_pruning_gd_top_k_mc_ac_dk_admm_retrain/\n"
     ]
    }
   ],
   "source": [
    "paths = [\n",
    "    '../logfiles/baseline/',\n",
    "    '../logfiles/experiments/re_pruning/',\n",
    "    '../logfiles/experiments/gd_top_k_mc_ac_dk/',\n",
    "    \n",
    "    '../logfiles/ablation_study/alexnet_mixed/',\n",
    "    '../logfiles/ablation_study/resnet_mixed/',\n",
    "    \n",
    "    '../logfiles/ablation_study/admm_intra/',\n",
    "    '../logfiles/ablation_study/admm_retrain/',\n",
    "    \n",
    "    '../logfiles/ablation_study/gd_top_k/',\n",
    "    '../logfiles/ablation_study/gd_top_k_mc/',\n",
    "    '../logfiles/ablation_study/gd_top_k_mc_ac/',\n",
    "    '../logfiles/ablation_study/gd_top_k_mc_ac_dk/',\n",
    "    '../logfiles/ablation_study/gd_top_k_mc_ac_dk_admm_intra/',\n",
    "    '../logfiles/ablation_study/gd_top_k_mc_ac_dk_admm_retrain/',\n",
    "    \n",
    "    '../logfiles/ablation_study/re_pruning/',\n",
    "    '../logfiles/ablation_study/re_pruning_admm_retrain/',\n",
    "    '../logfiles/ablation_study/re_pruning_admm_intra/',\n",
    "    '../logfiles/ablation_study/re_pruning_ac/',\n",
    "    '../logfiles/ablation_study/re_pruning_ac_admm_intra/',\n",
    "    '../logfiles/ablation_study/re_pruning_ac_admm_retrain/',\n",
    "    '../logfiles/ablation_study/re_pruning_gd_top_k_mc_ac_dk_admm_intra/',\n",
    "    '../logfiles/ablation_study/re_pruning_gd_top_k_mc_ac_dk_admm_retrain/'\n",
    "        ]\n",
    "logs = []\n",
    "for path in paths:\n",
    "    print(path, flush=True)\n",
    "    fnames = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    for fname in fnames:\n",
    "        if 'json' in fname:\n",
    "            with open(path+fname, 'r') as f:\n",
    "                logs.append(json.load(f))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "greek-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_eval(dataset, model, name, specs_to_print, results_to_print, plt_corr = True):\n",
    "    pd_dict = {}\n",
    "    for log in logs:\n",
    "        if (log['METADATA']['EXPERIMENT']['dataset'] == dataset and \n",
    "            log['METADATA']['EXPERIMENT']['name'] == name and\n",
    "            log['METADATA']['EXPERIMENT']['model'] == model):\n",
    "            outstring = model.upper() + ' ' + dataset.upper() + ' ' + name.upper() + '\\n'\n",
    "            for key in specs_to_print:\n",
    "                section = None\n",
    "                if key in log['METADATA']['SPECIFICATION']:\n",
    "                    section = 'SPECIFICATION'\n",
    "                if key in log['METADATA']['EXPERIMENT']:\n",
    "                    section = 'EXPERIMENT'\n",
    "                \n",
    "                outstring += key + ':' + log['METADATA'][section][key] + '\\n'\n",
    "                if key not in pd_dict:\n",
    "                    pd_dict[key] = []\n",
    "                pd_dict[key].append(float(log['METADATA'][section][key]))\n",
    "            #outstring += '\\n'\n",
    "            for key in results_to_print:\n",
    "                if type(log['LOGDATA'][key]) == type([]):\n",
    "                    outstring += key + ':' + str(round(log['LOGDATA'][key][-1], 2)) + '\\n'\n",
    "                    if key not in pd_dict:\n",
    "                        pd_dict[key] = []\n",
    "                    pd_dict[key].append(log['LOGDATA'][key][-1])\n",
    "                else:\n",
    "                    outstring += key + ':' + str(round(log['LOGDATA'][key], 2)) + '\\n'\n",
    "                    if key not in pd_dict:\n",
    "                        pd_dict[key] = []\n",
    "                    pd_dict[key].append(log['LOGDATA'][key])\n",
    "                    \n",
    "            #    if type(log['LOGDATA'][key]) == type([]):\n",
    "            #        plt.plot(log['LOGDATA'][key], label=key)\n",
    "            #plt.title(model.upper() + ' ' + dataset.upper() + ' ' + name.upper())\n",
    "            #plt.legend()\n",
    "            #plt.show()\n",
    "            outstring+='\\n'\n",
    "            print(outstring)\n",
    "                \n",
    "    if plt_corr:\n",
    "        pd_df = pd.DataFrame(pd_dict)\n",
    "        if len(pd_df) > 0:\n",
    "            pd_df = pd_df.loc[:, (pd_df != pd_df.iloc[0]).any()] #drop const cols\n",
    "            sns.heatmap(pd_df.corr(), cbar=True, annot=True, cmap='RdBu')\n",
    "            plt.title(model.upper() + ' ' + dataset.upper() + ' ' + name.upper())\n",
    "            plt.xticks(rotation=45) \n",
    "            plt.show()\n",
    "            \n",
    "def cross_eval(datasets, models, name, specs_to_print, results_to_print, plt_corr = True):\n",
    "    pd_dict = {}\n",
    "    res_dict = {}\n",
    "    best_config = {}\n",
    "    for model in models:\n",
    "        for dataset in datasets:\n",
    "            if model not in res_dict:\n",
    "                res_dict[model] = {}\n",
    "            if dataset not in res_dict[model]:\n",
    "                res_dict[model][dataset] = {}\n",
    "            if model not in best_config:\n",
    "                best_config[model] = {}\n",
    "            if dataset not in best_config[model]:\n",
    "                best_config[model][dataset] = {}\n",
    "            res_dict[model][dataset]['test_accuracy'] = 0.0\n",
    "            res_dict[model][dataset]['total_su'] = 1.0\n",
    "            res_dict[model][dataset]['current_su_fwd'] = 1.0\n",
    "            res_dict[model][dataset]['current_sparsity'] = 0.0\n",
    "            \n",
    "            for log in logs:\n",
    "                if (log['METADATA']['EXPERIMENT']['dataset'] == dataset and \n",
    "                    log['METADATA']['EXPERIMENT']['name'] == name and\n",
    "                    log['METADATA']['EXPERIMENT']['model'] == model):\n",
    "                    outstring = model.upper() + ' ' + dataset.upper() + ' ' + name.upper() + '\\n'\n",
    "                    \n",
    "                    if log['LOGDATA']['test_accuracy'][-1] > 1:\n",
    "                        log['LOGDATA']['test_accuracy'] = [x * 1e-2 for x in log['LOGDATA']['test_accuracy']]\n",
    "                        \n",
    "                    #TODO = selectable . in rounding\n",
    "                    if round(res_dict[model][dataset]['test_accuracy'],2) < round(log['LOGDATA']['test_accuracy'][-1],2):\n",
    "                        res_dict[model][dataset]['test_accuracy'] = log['LOGDATA']['test_accuracy'][-1]\n",
    "                        if name != 'baseline':\n",
    "                            res_dict[model][dataset]['current_su_fwd'] = log['LOGDATA']['current_su_fwd'][-1]\n",
    "                            res_dict[model][dataset]['total_su'] = log['LOGDATA']['total_su'][-1]\n",
    "                            res_dict[model][dataset]['current_sparsity'] = log['LOGDATA']['current_sparsity'][-1]\n",
    "                            best_config[model][dataset]['METADATA'] = log['METADATA']\n",
    "                    if round(res_dict[model][dataset]['test_accuracy'],2) == round(log['LOGDATA']['test_accuracy'][-1],2):\n",
    "                        if name != 'baseline':\n",
    "                            if round(res_dict[model][dataset]['total_su'],2) < round(log['LOGDATA']['total_su'][-1],2):\n",
    "                                res_dict[model][dataset]['current_su_fwd'] = log['LOGDATA']['current_su_fwd'][-1]\n",
    "                                res_dict[model][dataset]['test_accuracy'] = log['LOGDATA']['test_accuracy'][-1]\n",
    "                                res_dict[model][dataset]['total_su'] = log['LOGDATA']['total_su'][-1]\n",
    "                                res_dict[model][dataset]['current_sparsity'] = log['LOGDATA']['current_sparsity'][-1]\n",
    "                                best_config[model][dataset]['METADATA'] = log['METADATA']\n",
    "                    \n",
    "                    for key in specs_to_print:\n",
    "                        section = None\n",
    "                        if key in log['METADATA']['SPECIFICATION']:\n",
    "                            section = 'SPECIFICATION'\n",
    "                        if key in log['METADATA']['EXPERIMENT']:\n",
    "                            section = 'EXPERIMENT'\n",
    "\n",
    "                        outstring += key + ':' + log['METADATA'][section][key] + '\\n'\n",
    "                        if key not in pd_dict:\n",
    "                            pd_dict[key] = []\n",
    "                        pd_dict[key].append(float(log['METADATA'][section][key]))\n",
    "                    #outstring += '\\n'\n",
    "                    for key in results_to_print:\n",
    "                        if type(log['LOGDATA'][key]) == type([]):\n",
    "                            outstring += key + ':' + str(round(log['LOGDATA'][key][-1], 2)) + '\\n'\n",
    "                            if key not in pd_dict:\n",
    "                                pd_dict[key] = []\n",
    "                            pd_dict[key].append(log['LOGDATA'][key][-1])\n",
    "                        else:\n",
    "                            outstring += key + ':' + str(round(log['LOGDATA'][key], 2)) + '\\n'\n",
    "                            if key not in pd_dict:\n",
    "                                pd_dict[key] = []\n",
    "                            pd_dict[key].append(log['LOGDATA'][key])\n",
    "\n",
    "                    #    if type(log['LOGDATA'][key]) == type([]):\n",
    "                    #        plt.plot(log['LOGDATA'][key], label=key)\n",
    "                    #plt.title(model.upper() + ' ' + dataset.upper() + ' ' + name.upper())\n",
    "                    #plt.legend()\n",
    "                    #plt.show()\n",
    "                    outstring+='\\n'\n",
    "                    #print(outstring)\n",
    "    for model in res_dict:\n",
    "        for dataset in res_dict[model]:\n",
    "            if res_dict[model][dataset]['test_accuracy'] > 0.0:\n",
    "                #print(res_dict[model][dataset].keys())\n",
    "                print('EXP.: {}, MODEL: {}, DATA: {}'.format(name, model, dataset))\n",
    "                print('ACC.: {}, TRAIN SU: {}, INF. SU: {}, SP.: {}'.format(\n",
    "                      round(res_dict[model][dataset]['test_accuracy'],2),\n",
    "                      round(res_dict[model][dataset]['total_su'],2),\n",
    "                      round(res_dict[model][dataset]['current_su_fwd'],2),\n",
    "                      round(res_dict[model][dataset]['current_sparsity'],2)))\n",
    "                #print(best_config[model][dataset])\n",
    "    print('\\n\\n')\n",
    "    if plt_corr:\n",
    "        pd_df = pd.DataFrame(pd_dict)\n",
    "        if len(pd_df) > 0:\n",
    "            pd_df = pd_df.loc[:, (pd_df != pd_df.iloc[0]).any()] #drop const cols\n",
    "            sns.heatmap(pd_df.corr(), cbar=True, annot=True, cmap='RdBu')\n",
    "            plt.title(name.upper())\n",
    "            plt.xticks(rotation=90) \n",
    "            plt.show()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "engaged-choice",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXP.: baseline, MODEL: resnet18, DATA: cifar10\n",
      "ACC.: 0.75, TRAIN SU: 1.0, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: baseline, MODEL: resnet18, DATA: cifar100\n",
      "ACC.: 0.43, TRAIN SU: 1.0, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: baseline, MODEL: resnet20, DATA: cifar10\n",
      "ACC.: 0.8, TRAIN SU: 1.0, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: baseline, MODEL: resnet32, DATA: cifar10\n",
      "ACC.: 0.81, TRAIN SU: 1.0, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: baseline, MODEL: resnet32, DATA: cifar100\n",
      "ACC.: 0.47, TRAIN SU: 1.0, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: baseline, MODEL: resnet50, DATA: cifar10\n",
      "ACC.: 0.74, TRAIN SU: 1.0, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: baseline, MODEL: alexnet_s, DATA: cifar10\n",
      "ACC.: 0.74, TRAIN SU: 1.0, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: baseline, MODEL: alexnet_s, DATA: cifar100\n",
      "ACC.: 0.37, TRAIN SU: 1.0, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: baseline, MODEL: lenet, DATA: mnist\n",
      "ACC.: 0.99, TRAIN SU: 1.0, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: baseline, MODEL: mobilenet_v2, DATA: cifar10\n",
      "ACC.: 0.77, TRAIN SU: 1.0, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: baseline, MODEL: mobilenet_v2, DATA: cifar100\n",
      "ACC.: 0.43, TRAIN SU: 1.0, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: baseline, MODEL: vgg8, DATA: cifar10\n",
      "ACC.: 0.77, TRAIN SU: 1.0, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: baseline, MODEL: vgg8, DATA: cifar100\n",
      "ACC.: 0.46, TRAIN SU: 1.0, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: baseline, MODEL: vgg11, DATA: cifar10\n",
      "ACC.: 0.76, TRAIN SU: 1.0, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: baseline, MODEL: vgg11, DATA: cifar100\n",
      "ACC.: 0.36, TRAIN SU: 1.0, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: baseline, MODEL: vgg13, DATA: cifar10\n",
      "ACC.: 0.76, TRAIN SU: 1.0, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: baseline, MODEL: vgg13, DATA: cifar100\n",
      "ACC.: 0.28, TRAIN SU: 1.0, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: baseline, MODEL: vgg16, DATA: cifar10\n",
      "ACC.: 0.72, TRAIN SU: 1.0, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: baseline, MODEL: vgg16, DATA: cifar100\n",
      "ACC.: 0.26, TRAIN SU: 1.0, INF. SU: 1.0, SP.: 0.0\n",
      "\n",
      "\n",
      "\n",
      "EXP.: admm_intra, MODEL: resnet18, DATA: cifar10\n",
      "ACC.: 0.75, TRAIN SU: 2.48, INF. SU: 1.86, SP.: 0.71\n",
      "EXP.: admm_intra, MODEL: alexnet_s, DATA: cifar10\n",
      "ACC.: 0.72, TRAIN SU: 2.17, INF. SU: 1.86, SP.: 0.98\n",
      "EXP.: admm_intra, MODEL: lenet, DATA: mnist\n",
      "ACC.: 0.99, TRAIN SU: 2.03, INF. SU: 12.01, SP.: 0.99\n",
      "\n",
      "\n",
      "\n",
      "EXP.: admm_retrain, MODEL: resnet18, DATA: cifar10\n",
      "ACC.: 0.7, TRAIN SU: 1.32, INF. SU: 1.86, SP.: 0.71\n",
      "EXP.: admm_retrain, MODEL: alexnet_s, DATA: cifar10\n",
      "ACC.: 0.7, TRAIN SU: 1.53, INF. SU: 1.86, SP.: 0.98\n",
      "EXP.: admm_retrain, MODEL: lenet, DATA: mnist\n",
      "ACC.: 0.99, TRAIN SU: 1.24, INF. SU: 12.01, SP.: 0.99\n",
      "\n",
      "\n",
      "\n",
      "EXP.: gd_top_k, MODEL: resnet18, DATA: cifar10\n",
      "ACC.: 0.76, TRAIN SU: 1.24, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: gd_top_k, MODEL: alexnet_s, DATA: cifar10\n",
      "ACC.: 0.67, TRAIN SU: 1.7, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: gd_top_k, MODEL: lenet, DATA: mnist\n",
      "ACC.: 0.99, TRAIN SU: 2.07, INF. SU: 1.0, SP.: 0.0\n",
      "\n",
      "\n",
      "\n",
      "EXP.: gd_top_k_mc, MODEL: resnet18, DATA: cifar10\n",
      "ACC.: 0.76, TRAIN SU: 1.23, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: gd_top_k_mc, MODEL: alexnet_s, DATA: cifar10\n",
      "ACC.: 0.66, TRAIN SU: 1.7, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: gd_top_k_mc, MODEL: lenet, DATA: mnist\n",
      "ACC.: 0.99, TRAIN SU: 1.73, INF. SU: 1.0, SP.: 0.0\n",
      "\n",
      "\n",
      "\n",
      "EXP.: gd_top_k_mc_ac, MODEL: resnet18, DATA: cifar10\n",
      "ACC.: 0.75, TRAIN SU: 1.59, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: gd_top_k_mc_ac, MODEL: alexnet_s, DATA: cifar10\n",
      "ACC.: 0.69, TRAIN SU: 2.03, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: gd_top_k_mc_ac, MODEL: lenet, DATA: mnist\n",
      "ACC.: 0.99, TRAIN SU: 2.32, INF. SU: 1.0, SP.: 0.0\n",
      "\n",
      "\n",
      "\n",
      "EXP.: gd_top_k_mc_ac_dk, MODEL: resnet18, DATA: cifar10\n",
      "ACC.: 0.77, TRAIN SU: 1.46, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: gd_top_k_mc_ac_dk, MODEL: resnet18, DATA: cifar100\n",
      "ACC.: 0.42, TRAIN SU: 1.41, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: gd_top_k_mc_ac_dk, MODEL: alexnet_s, DATA: cifar10\n",
      "ACC.: 0.7, TRAIN SU: 1.65, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: gd_top_k_mc_ac_dk, MODEL: alexnet_s, DATA: cifar100\n",
      "ACC.: 0.37, TRAIN SU: 1.51, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: gd_top_k_mc_ac_dk, MODEL: lenet, DATA: mnist\n",
      "ACC.: 0.96, TRAIN SU: 2.31, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: gd_top_k_mc_ac_dk, MODEL: mobilenet_v2, DATA: cifar10\n",
      "ACC.: 0.7, TRAIN SU: 1.42, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: gd_top_k_mc_ac_dk, MODEL: mobilenet_v2, DATA: cifar100\n",
      "ACC.: 0.32, TRAIN SU: 1.41, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: gd_top_k_mc_ac_dk, MODEL: vgg8, DATA: cifar10\n",
      "ACC.: 0.75, TRAIN SU: 1.52, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: gd_top_k_mc_ac_dk, MODEL: vgg8, DATA: cifar100\n",
      "ACC.: 0.39, TRAIN SU: 1.45, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: gd_top_k_mc_ac_dk, MODEL: vgg11, DATA: cifar10\n",
      "ACC.: 0.76, TRAIN SU: 1.48, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: gd_top_k_mc_ac_dk, MODEL: vgg11, DATA: cifar100\n",
      "ACC.: 0.35, TRAIN SU: 1.44, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: gd_top_k_mc_ac_dk, MODEL: vgg13, DATA: cifar10\n",
      "ACC.: 0.77, TRAIN SU: 1.46, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: gd_top_k_mc_ac_dk, MODEL: vgg13, DATA: cifar100\n",
      "ACC.: 0.38, TRAIN SU: 1.44, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: gd_top_k_mc_ac_dk, MODEL: vgg16, DATA: cifar10\n",
      "ACC.: 0.8, TRAIN SU: 1.42, INF. SU: 1.0, SP.: 0.0\n",
      "EXP.: gd_top_k_mc_ac_dk, MODEL: vgg16, DATA: cifar100\n",
      "ACC.: 0.37, TRAIN SU: 1.45, INF. SU: 1.0, SP.: 0.0\n",
      "\n",
      "\n",
      "\n",
      "EXP.: gd_top_k_mc_ac_dk_admm_intra, MODEL: resnet18, DATA: cifar10\n",
      "ACC.: 0.75, TRAIN SU: 2.98, INF. SU: 1.86, SP.: 0.71\n",
      "EXP.: gd_top_k_mc_ac_dk_admm_intra, MODEL: alexnet_s, DATA: cifar10\n",
      "ACC.: 0.7, TRAIN SU: 2.8, INF. SU: 1.86, SP.: 0.98\n",
      "EXP.: gd_top_k_mc_ac_dk_admm_intra, MODEL: lenet, DATA: mnist\n",
      "ACC.: 0.98, TRAIN SU: 4.39, INF. SU: 12.01, SP.: 0.99\n",
      "\n",
      "\n",
      "\n",
      "EXP.: gd_top_k_mc_ac_dk_admm_retrain, MODEL: lenet, DATA: mnist\n",
      "ACC.: 0.98, TRAIN SU: 2.58, INF. SU: 12.01, SP.: 0.99\n",
      "\n",
      "\n",
      "\n",
      "EXP.: re_pruning, MODEL: resnet18, DATA: cifar10\n",
      "ACC.: 0.76, TRAIN SU: 1.22, INF. SU: 1.27, SP.: 0.96\n",
      "EXP.: re_pruning, MODEL: resnet18, DATA: cifar100\n",
      "ACC.: 0.42, TRAIN SU: 1.16, INF. SU: 1.19, SP.: 0.9\n",
      "EXP.: re_pruning, MODEL: alexnet_s, DATA: cifar10\n",
      "ACC.: 0.65, TRAIN SU: 1.35, INF. SU: 1.37, SP.: 0.23\n",
      "EXP.: re_pruning, MODEL: alexnet_s, DATA: cifar100\n",
      "ACC.: 0.34, TRAIN SU: 1.35, INF. SU: 1.37, SP.: 0.24\n",
      "EXP.: re_pruning, MODEL: lenet, DATA: mnist\n",
      "ACC.: 0.97, TRAIN SU: 39.95, INF. SU: 19.48, SP.: 0.99\n",
      "EXP.: re_pruning, MODEL: mobilenet_v2, DATA: cifar10\n",
      "ACC.: 0.75, TRAIN SU: 1.19, INF. SU: 1.29, SP.: 0.79\n",
      "EXP.: re_pruning, MODEL: mobilenet_v2, DATA: cifar100\n",
      "ACC.: 0.36, TRAIN SU: 1.08, INF. SU: 1.09, SP.: 0.53\n",
      "EXP.: re_pruning, MODEL: vgg8, DATA: cifar10\n",
      "ACC.: 0.1, TRAIN SU: 1.95, INF. SU: 2.0, SP.: 1.0\n",
      "EXP.: re_pruning, MODEL: vgg8, DATA: cifar100\n",
      "ACC.: 0.01, TRAIN SU: 1.95, INF. SU: 2.0, SP.: 1.0\n",
      "EXP.: re_pruning, MODEL: vgg11, DATA: cifar10\n",
      "ACC.: 0.7, TRAIN SU: 1.35, INF. SU: 1.41, SP.: 0.3\n",
      "EXP.: re_pruning, MODEL: vgg11, DATA: cifar100\n",
      "ACC.: 0.01, TRAIN SU: 1.95, INF. SU: 2.0, SP.: 1.0\n",
      "EXP.: re_pruning, MODEL: vgg13, DATA: cifar10\n",
      "ACC.: 0.1, TRAIN SU: 1.95, INF. SU: 2.0, SP.: 1.0\n",
      "EXP.: re_pruning, MODEL: vgg13, DATA: cifar100\n",
      "ACC.: 0.01, TRAIN SU: 1.96, INF. SU: 2.0, SP.: 1.0\n",
      "EXP.: re_pruning, MODEL: vgg16, DATA: cifar10\n",
      "ACC.: 0.1, TRAIN SU: 1.95, INF. SU: 2.0, SP.: 1.0\n",
      "EXP.: re_pruning, MODEL: vgg16, DATA: cifar100\n",
      "ACC.: 0.01, TRAIN SU: 1.94, INF. SU: 2.0, SP.: 1.0\n",
      "\n",
      "\n",
      "\n",
      "EXP.: re_pruning_ac, MODEL: resnet18, DATA: cifar10\n",
      "ACC.: 0.74, TRAIN SU: 1.57, INF. SU: 1.2, SP.: 0.92\n",
      "EXP.: re_pruning_ac, MODEL: alexnet_s, DATA: cifar10\n",
      "ACC.: 0.67, TRAIN SU: 1.89, INF. SU: 1.39, SP.: 0.2\n",
      "EXP.: re_pruning_ac, MODEL: lenet, DATA: mnist\n",
      "ACC.: 0.97, TRAIN SU: 59.11, INF. SU: 23.62, SP.: 0.99\n",
      "\n",
      "\n",
      "\n",
      "EXP.: re_pruning_admm_intra, MODEL: alexnet_s, DATA: cifar10\n",
      "ACC.: 0.69, TRAIN SU: 2.63, INF. SU: 1.87, SP.: 0.99\n",
      "EXP.: re_pruning_admm_intra, MODEL: lenet, DATA: mnist\n",
      "ACC.: 0.99, TRAIN SU: 13.7, INF. SU: 12.02, SP.: 0.99\n",
      "\n",
      "\n",
      "\n",
      "EXP.: re_pruning_admm_retrain, MODEL: alexnet_s, DATA: cifar10\n",
      "ACC.: 0.68, TRAIN SU: 2.27, INF. SU: 1.87, SP.: 0.99\n",
      "EXP.: re_pruning_admm_retrain, MODEL: lenet, DATA: mnist\n",
      "ACC.: 0.97, TRAIN SU: 15.05, INF. SU: 14.97, SP.: 0.99\n",
      "\n",
      "\n",
      "\n",
      "EXP.: re_pruning_ac_admm_intra, MODEL: alexnet_s, DATA: cifar10\n",
      "ACC.: 0.69, TRAIN SU: 2.82, INF. SU: 1.86, SP.: 0.98\n",
      "EXP.: re_pruning_ac_admm_intra, MODEL: lenet, DATA: mnist\n",
      "ACC.: 0.99, TRAIN SU: 11.77, INF. SU: 12.03, SP.: 0.99\n",
      "\n",
      "\n",
      "\n",
      "EXP.: re_pruning_ac_admm_retrain, MODEL: alexnet_s, DATA: cifar10\n",
      "ACC.: 0.66, TRAIN SU: 2.53, INF. SU: 1.87, SP.: 0.99\n",
      "EXP.: re_pruning_ac_admm_retrain, MODEL: lenet, DATA: mnist\n",
      "ACC.: 0.98, TRAIN SU: 17.35, INF. SU: 12.21, SP.: 0.99\n",
      "\n",
      "\n",
      "\n",
      "EXP.: re_pruning_gd_top_k_mc_ac_dk_admm_intra, MODEL: alexnet_s, DATA: cifar10\n",
      "ACC.: 0.7, TRAIN SU: 3.08, INF. SU: 1.86, SP.: 0.98\n",
      "EXP.: re_pruning_gd_top_k_mc_ac_dk_admm_intra, MODEL: lenet, DATA: mnist\n",
      "ACC.: 0.98, TRAIN SU: 18.59, INF. SU: 12.1, SP.: 0.99\n",
      "\n",
      "\n",
      "\n",
      "EXP.: re_pruning_gd_top_k_mc_ac_dk_admm_retrain, MODEL: alexnet_s, DATA: cifar10\n",
      "ACC.: 0.71, TRAIN SU: 3.25, INF. SU: 1.87, SP.: 0.99\n",
      "EXP.: re_pruning_gd_top_k_mc_ac_dk_admm_retrain, MODEL: lenet, DATA: mnist\n",
      "ACC.: 0.97, TRAIN SU: 14.8, INF. SU: 12.75, SP.: 0.99\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "    'cifar10', \n",
    "    'cifar100', \n",
    "    'mnist',\n",
    "    'imagenet_tiny',\n",
    "    'imagenet_full',\n",
    "]\n",
    "models = [\n",
    "    'resnet18', \n",
    "    'resnet20',\n",
    "    'resnet32',\n",
    "    'resnet34',\n",
    "    'resnet50',\n",
    "    'alexnet_s', \n",
    "    'alexnet',\n",
    "    'lenet', \n",
    "    'mobilenet_v2', \n",
    "    'mobilenet_v3', \n",
    "    'vgg8',\n",
    "    'vgg11', \n",
    "    'vgg13', \n",
    "    'vgg16'\n",
    "         ]\n",
    "plt_corr = False\n",
    "\n",
    "cross_eval(datasets, models, 'baseline', ['lr', 'epochs', 'train_batch_size'], ['test_accuracy'], plt_corr)\n",
    "\n",
    "\n",
    "cross_eval(datasets, models, 'admm_intra', \n",
    "     ['lr', 'pre_epochs', 'epochs', 're_epochs', 'repeat', 'train_batch_size'], \n",
    "     ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', 'current_sparsity', \n",
    "      'current_relative_overhead'], plt_corr)\n",
    "cross_eval(datasets, models, 'admm_retrain', \n",
    "     ['lr', 'pre_epochs', 'epochs', 're_epochs', 'train_batch_size'], \n",
    "     ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', 'current_sparsity', \n",
    "      'current_relative_overhead'], plt_corr)\n",
    "\n",
    "cross_eval(datasets, models, 'gd_top_k', ['lr', 'k', 'train_batch_size'], \n",
    "             ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', 'current_sparsity', \n",
    "              'current_relative_overhead'], plt_corr)\n",
    "cross_eval(datasets, models, 'gd_top_k_mc', ['lr', 'k', 'se', 'train_batch_size'], \n",
    "     ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', 'current_sparsity', \n",
    "      'current_relative_overhead'], plt_corr)\n",
    "cross_eval(datasets, models, 'gd_top_k_mc_ac', ['lr', 'k', 'se', 'ac', 'train_batch_size'], \n",
    "     ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', 'current_sparsity', \n",
    "      'current_relative_overhead'], plt_corr)\n",
    "cross_eval(datasets, models, 'gd_top_k_mc_ac_dk', ['lr', 'k', 'se', 'ac', 'train_batch_size'], \n",
    "     ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', 'current_sparsity', \n",
    "      'current_relative_overhead'], plt_corr)\n",
    "cross_eval(datasets, models, 'gd_top_k_mc_ac_dk_admm_intra', \n",
    "     ['lr', 'k', 'se', 'ac', 'pre_epochs', 'epochs', 're_epochs', 'repeat', 'train_batch_size'], \n",
    "     ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', 'current_sparsity', \n",
    "      'current_relative_overhead'], plt_corr)\n",
    "cross_eval(datasets, models, 'gd_top_k_mc_ac_dk_admm_retrain',  \n",
    "     ['lr', 'k', 'se', 'ac', 'pre_epochs', 'epochs', 're_epochs', 'train_batch_size'], \n",
    "     ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', 'current_sparsity', \n",
    "      'current_relative_overhead'], plt_corr)\n",
    "\n",
    "\n",
    "cross_eval(datasets, models, 're_pruning', ['lr', 'prune_epochs', 'metric_q_l', 'metric_q_c', 'scale_l', 'scale_c',\n",
    "      'sample_l', 'sample_c', 'softness_l', 'softness_c', 'magnitude_t_c', 'magnitude_t_l', 'l1', 'l2', \n",
    "      'train_batch_size'], ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', \n",
    "      'current_sparsity', 'current_relative_overhead'], plt_corr)\n",
    "cross_eval(datasets, models, 're_pruning_ac', ['lr', 'prune_epochs', 'metric_q_l', 'metric_q_c', 'scale_l', 'scale_c',\n",
    "      'sample_l', 'sample_c', 'softness_l', 'softness_c', 'magnitude_t_c', 'magnitude_t_l', 'l1', 'l2', 'ac', \n",
    "      'train_batch_size'], ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', \n",
    "      'current_sparsity', 'current_relative_overhead'], plt_corr)\n",
    "\n",
    "\n",
    "cross_eval(datasets, models, 're_pruning_admm_intra', ['lr', 'prune_epochs', 'metric_q_l', 'metric_q_c', 'scale_l', 'scale_c',\n",
    "      'sample_l', 'sample_c', 'softness_l', 'softness_c', 'magnitude_t_c', 'magnitude_t_l', 'repeat',\n",
    "      'train_batch_size'], ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', \n",
    "      'current_sparsity', 'current_relative_overhead'], plt_corr)\n",
    "cross_eval(datasets, models, 're_pruning_admm_retrain', ['lr', 'prune_epochs', 'metric_q_l', 'metric_q_c', 'scale_l', 'scale_c',\n",
    "      'sample_l', 'sample_c', 'softness_l', 'softness_c', 'magnitude_t_c', 'magnitude_t_l', \n",
    "      'train_batch_size'], ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', \n",
    "      'current_sparsity', 'current_relative_overhead'], plt_corr)\n",
    "cross_eval(datasets, models, 're_pruning_ac_admm_intra', ['lr', 'prune_epochs', 'metric_q_l', 'metric_q_c', 'scale_l', 'scale_c',\n",
    "      'sample_l', 'sample_c', 'softness_l', 'softness_c', 'magnitude_t_c', 'magnitude_t_l', 'ac', 'repeat',\n",
    "      'train_batch_size'], ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', \n",
    "      'current_sparsity', 'current_relative_overhead'], plt_corr)\n",
    "cross_eval(datasets, models, 're_pruning_ac_admm_retrain', ['lr', 'prune_epochs', 'metric_q_l', 'metric_q_c', 'scale_l', 'scale_c',\n",
    "      'sample_l', 'sample_c', 'softness_l', 'softness_c', 'magnitude_t_c', 'magnitude_t_l', 'ac', \n",
    "      'train_batch_size'], ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', \n",
    "      'current_sparsity', 'current_relative_overhead'], plt_corr)\n",
    "\n",
    "\n",
    "\n",
    "cross_eval(datasets, models, 're_pruning_gd_top_k_mc_ac_dk_admm_intra', ['lr', 'prune_epochs', 'metric_q_l', \n",
    "      'metric_q_c', 'scale_l', 'scale_c', 'sample_l', 'sample_c', 'softness_l', 'softness_c', 'magnitude_t_c', \n",
    "      'magnitude_t_l', 'ac', 'train_batch_size'], ['test_accuracy', 'total_su', 'total_su_fwd', \n",
    "      'total_su_bwd', 'current_sparsity', 'current_relative_overhead'], False)\n",
    "\n",
    "cross_eval(datasets, models, 're_pruning_gd_top_k_mc_ac_dk_admm_retrain', ['lr', 'prune_epochs', 'metric_q_l', \n",
    "      'metric_q_c', 'scale_l', 'scale_c', 'sample_l', 'sample_c', 'softness_l', 'softness_c', 'magnitude_t_c', \n",
    "      'magnitude_t_l', 'ac', 'train_batch_size'], ['test_accuracy', 'total_su', 'total_su_fwd', \n",
    "      'total_su_bwd', 'current_sparsity', 'current_relative_overhead'], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-envelope",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    'cifar10', \n",
    "    'cifar100', \n",
    "    'mnist',\n",
    "    'imagenet'\n",
    "]\n",
    "models = [\n",
    "    'resnet18', \n",
    "    'alexnet_s', \n",
    "    'lenet', \n",
    "    'mobilenet_v2', \n",
    "    'mobilenet_v3', \n",
    "    'vgg8',\n",
    "    'vgg11', \n",
    "    'vgg13', \n",
    "    'vgg16'\n",
    "         ]\n",
    "plt_corr = False\n",
    "\n",
    "for model in models:\n",
    "    for dataset in datasets:\n",
    "        single_eval(dataset, model, 'baseline', ['lr', 'epochs', 'train_batch_size'], ['test_accuracy'], plt_corr)\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    for dataset in datasets:\n",
    "        single_eval(dataset, model, 'admm_intra', \n",
    "             ['lr', 'pre_epochs', 'epochs', 're_epochs', 'repeat', 'train_batch_size'], \n",
    "             ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', 'current_sparsity', \n",
    "              'current_relative_overhead'], plt_corr)\n",
    "        single_eval(dataset, model, 'admm_retrain', \n",
    "             ['lr', 'pre_epochs', 'epochs', 're_epochs', 'train_batch_size'], \n",
    "             ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', 'current_sparsity', \n",
    "              'current_relative_overhead'], plt_corr)\n",
    "\n",
    "for model in models:\n",
    "    for dataset in datasets:\n",
    "        single_eval(dataset, model, 'gd_top_k', ['lr', 'k', 'train_batch_size'], \n",
    "             ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', 'current_sparsity', \n",
    "              'current_relative_overhead'], plt_corr)\n",
    "        single_eval(dataset, model, 'gd_top_k_mc', ['lr', 'k', 'se', 'train_batch_size'], \n",
    "             ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', 'current_sparsity', \n",
    "              'current_relative_overhead'], plt_corr)\n",
    "        single_eval(dataset, model, 'gd_top_k_mc_ac', ['lr', 'k', 'se', 'ac', 'train_batch_size'], \n",
    "             ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', 'current_sparsity', \n",
    "              'current_relative_overhead'], plt_corr)\n",
    "        single_eval(dataset, model, 'gd_top_k_mc_ac_dk', ['lr', 'k', 'se', 'ac', 'train_batch_size'], \n",
    "             ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', 'current_sparsity', \n",
    "              'current_relative_overhead'], plt_corr)\n",
    "        single_eval(dataset, model, 'gd_top_k_mc_ac_dk_admm_intra', \n",
    "             ['lr', 'k', 'se', 'ac', 'pre_epochs', 'epochs', 're_epochs', 'repeat', 'train_batch_size'], \n",
    "             ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', 'current_sparsity', \n",
    "              'current_relative_overhead'], plt_corr)\n",
    "        single_eval(dataset, model, 'gd_top_k_mc_ac_dk_admm_retrain',  \n",
    "             ['lr', 'k', 'se', 'ac', 'pre_epochs', 'epochs', 're_epochs', 'train_batch_size'], \n",
    "             ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', 'current_sparsity', \n",
    "              'current_relative_overhead'], plt_corr)\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    for dataset in datasets:\n",
    "        single_eval(dataset, model, 're_pruning', ['lr', 'prune_epochs', 'metric_q_l', 'metric_q_c', 'scale_l', 'scale_c',\n",
    "              'sample_l', 'sample_c', 'softness_l', 'softness_c', 'magnitude_t_c', 'magnitude_t_l', 'l1', 'l2', \n",
    "              'train_batch_size'], ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', \n",
    "              'current_sparsity', 'current_relative_overhead'], plt_corr)\n",
    "        single_eval(dataset, model, 're_pruning_ac', ['lr', 'prune_epochs', 'metric_q_l', 'metric_q_c', 'scale_l', 'scale_c',\n",
    "              'sample_l', 'sample_c', 'softness_l', 'softness_c', 'magnitude_t_c', 'magnitude_t_l', 'l1', 'l2', 'ac', \n",
    "              'train_batch_size'], ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', \n",
    "              'current_sparsity', 'current_relative_overhead'], plt_corr)\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    for dataset in datasets:\n",
    "        single_eval(dataset, model, 're_pruning_admm_intra', ['lr', 'prune_epochs', 'metric_q_l', 'metric_q_c', 'scale_l', 'scale_c',\n",
    "              'sample_l', 'sample_c', 'softness_l', 'softness_c', 'magnitude_t_c', 'magnitude_t_l', 'repeat',\n",
    "              'train_batch_size'], ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', \n",
    "              'current_sparsity', 'current_relative_overhead'], plt_corr)\n",
    "        single_eval(dataset, model, 're_pruning_admm_retrain', ['lr', 'prune_epochs', 'metric_q_l', 'metric_q_c', 'scale_l', 'scale_c',\n",
    "              'sample_l', 'sample_c', 'softness_l', 'softness_c', 'magnitude_t_c', 'magnitude_t_l', \n",
    "              'train_batch_size'], ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', \n",
    "              'current_sparsity', 'current_relative_overhead'], plt_corr)\n",
    "        single_eval(dataset, model, 're_pruning_ac_admm_intra', ['lr', 'prune_epochs', 'metric_q_l', 'metric_q_c', 'scale_l', 'scale_c',\n",
    "              'sample_l', 'sample_c', 'softness_l', 'softness_c', 'magnitude_t_c', 'magnitude_t_l', 'ac', 'repeat',\n",
    "              'train_batch_size'], ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', \n",
    "              'current_sparsity', 'current_relative_overhead'], plt_corr)\n",
    "        single_eval(dataset, model, 're_pruning_ac_admm_retrain', ['lr', 'prune_epochs', 'metric_q_l', 'metric_q_c', 'scale_l', 'scale_c',\n",
    "              'sample_l', 'sample_c', 'softness_l', 'softness_c', 'magnitude_t_c', 'magnitude_t_l', 'ac', \n",
    "              'train_batch_size'], ['test_accuracy', 'total_su', 'total_su_fwd', 'total_su_bwd', \n",
    "              'current_sparsity', 'current_relative_overhead'], plt_corr)\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    for dataset in datasets:\n",
    "        single_eval(dataset, model, 're_pruning_gd_top_k_mc_ac_dk_admm_intra', ['lr', 'prune_epochs', 'metric_q_l', \n",
    "              'metric_q_c', 'scale_l', 'scale_c', 'sample_l', 'sample_c', 'softness_l', 'softness_c', 'magnitude_t_c', \n",
    "              'magnitude_t_l', 'l1', 'l2', 'ac', 'train_batch_size'], ['test_accuracy', 'total_su', 'total_su_fwd', \n",
    "              'total_su_bwd', 'current_sparsity', 'current_relative_overhead'], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "american-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "approved-forth",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar100+re_pruning+alexnet_s\n",
      "0.34 0.26 0.24\n",
      "\n",
      "\n",
      "cifar10+re_pruning+alexnet_s\n",
      "0.1 0.49 1.0\n",
      "0.1 0.49 1.0\n",
      "0.1 0.49 1.0\n",
      "0.1 0.49 1.0\n",
      "0.1 0.49 1.0\n",
      "0.1 0.49 1.0\n",
      "0.1 0.47 1.0\n",
      "0.57 0.37 0.22\n",
      "0.65 0.26 0.23\n",
      "0.56 0.26 0.21\n",
      "0.69 0.21 0.42\n",
      "0.41 0.15 0.14\n",
      "\n",
      "\n",
      "mnist+re_pruning+lenet\n",
      "0.97 0.97 0.99\n",
      "0.96 0.96 0.99\n",
      "0.97 0.95 0.99\n",
      "0.98 0.82 0.39\n",
      "0.99 0.79 0.14\n",
      "0.99 0.74 0.21\n",
      "0.99 0.68 0.09\n",
      "0.99 0.61 0.89\n",
      "0.98 0.55 0.13\n",
      "\n",
      "\n",
      "cifar100+re_pruning+resnet18\n",
      "0.42 0.14 0.9\n",
      "\n",
      "\n",
      "cifar10+re_pruning+resnet18\n",
      "0.1 0.29 0.98\n",
      "0.1 0.28 0.98\n",
      "0.1 0.28 0.98\n",
      "0.1 0.26 0.98\n",
      "0.74 0.24 0.96\n",
      "0.76 0.18 0.96\n",
      "0.75 0.18 0.96\n",
      "0.74 0.15 0.93\n",
      "0.74 0.15 0.93\n",
      "0.75 0.15 0.93\n",
      "0.75 0.14 0.92\n",
      "0.72 0.08 0.86\n",
      "\n",
      "\n",
      "cifar100+gd_top_k_mc_ac_dk+alexnet_s\n",
      "0.37 0.34 0.0\n",
      "\n",
      "\n",
      "cifar10+gd_top_k_mc_ac_dk+alexnet_s\n",
      "0.7 0.39 0.0\n",
      "0.73 0.34 0.0\n",
      "0.73 0.32 0.0\n",
      "0.73 0.32 0.0\n",
      "\n",
      "\n",
      "mnist+gd_top_k_mc_ac_dk+lenet\n",
      "0.93 0.6 0.0\n",
      "0.96 0.57 0.0\n",
      "0.99 0.55 0.0\n",
      "0.99 0.5 0.0\n",
      "0.99 0.49 0.0\n",
      "0.99 0.45 0.0\n",
      "0.99 0.43 0.0\n",
      "0.99 0.26 0.0\n",
      "0.99 0.24 0.0\n",
      "\n",
      "\n",
      "cifar100+gd_top_k_mc_ac_dk+resnet18\n",
      "0.42 0.29 0.0\n",
      "\n",
      "\n",
      "cifar10+gd_top_k_mc_ac_dk+resnet18\n",
      "0.77 0.32 0.0\n",
      "0.76 0.3 0.0\n",
      "0.76 0.3 0.0\n",
      "0.77 0.29 0.0\n",
      "0.77 0.29 0.0\n",
      "0.76 0.29 0.0\n",
      "0.76 0.28 0.0\n",
      "\n",
      "\n",
      "cifar10+admm_intra+alexnet_s\n",
      "0.72 0.54 0.98\n",
      "0.72 0.51 0.98\n",
      "0.71 0.43 0.98\n",
      "\n",
      "\n",
      "cifar10+gd_top_k_mc_ac+alexnet_s\n",
      "0.69 0.51 0.0\n",
      "0.69 0.51 0.0\n",
      "0.69 0.5 0.0\n",
      "0.69 0.5 0.0\n",
      "0.71 0.45 0.0\n",
      "0.71 0.45 0.0\n",
      "\n",
      "\n",
      "cifar10+admm_retrain+alexnet_s\n",
      "0.7 0.35 0.98\n",
      "0.7 0.31 0.98\n",
      "0.69 0.2 0.98\n",
      "\n",
      "\n",
      "cifar10+gd_top_k+alexnet_s\n",
      "0.61 0.46 0.0\n",
      "0.67 0.41 0.0\n",
      "0.7 0.4 0.0\n",
      "0.67 0.4 0.0\n",
      "0.72 0.33 0.0\n",
      "0.73 0.32 0.0\n",
      "\n",
      "\n",
      "cifar10+gd_top_k_mc_ac_dk_admm_intra+alexnet_s\n",
      "0.7 0.64 0.98\n",
      "0.73 0.63 0.98\n",
      "0.72 0.6 0.98\n",
      "0.72 0.6 0.98\n",
      "0.73 0.56 0.92\n",
      "\n",
      "\n",
      "cifar10+gd_top_k_mc+alexnet_s\n",
      "0.66 0.41 0.0\n",
      "0.66 0.39 0.0\n",
      "0.71 0.38 0.0\n",
      "0.71 0.38 0.0\n",
      "0.72 0.32 0.0\n",
      "0.72 0.32 0.0\n",
      "\n",
      "\n",
      "cifar10+re_pruning_ac+alexnet_s\n",
      "0.1 0.69 1.0\n",
      "0.67 0.47 0.2\n",
      "0.67 0.45 0.2\n",
      "0.63 0.43 0.19\n",
      "\n",
      "\n",
      "cifar10+re_pruning_ac_admm_intra+alexnet_s\n",
      "0.1 0.74 1.0\n",
      "0.69 0.65 0.98\n",
      "0.68 0.64 0.98\n",
      "0.71 0.64 0.98\n",
      "\n",
      "\n",
      "cifar10+re_pruning_ac_admm_retrain+alexnet_s\n",
      "0.66 0.61 0.99\n",
      "0.66 0.59 0.99\n",
      "0.6 0.55 0.99\n",
      "0.61 0.54 0.98\n",
      "\n",
      "\n",
      "cifar10+re_pruning_admm_intra+alexnet_s\n",
      "0.69 0.62 0.99\n",
      "0.7 0.61 0.99\n",
      "0.7 0.58 0.98\n",
      "0.7 0.58 0.98\n",
      "\n",
      "\n",
      "cifar10+re_pruning_admm_retrain+alexnet_s\n",
      "0.68 0.56 0.99\n",
      "0.1 0.49 1.0\n",
      "0.1 0.45 1.0\n",
      "0.66 0.42 0.99\n",
      "\n",
      "\n",
      "cifar10+re_pruning_gd_top_k_mc_ac_dk_admm_intra+alexnet_s\n",
      "0.1 0.82 1.0\n",
      "0.1 0.82 1.0\n",
      "0.7 0.68 0.98\n",
      "0.7 0.67 0.98\n",
      "0.7 0.66 0.98\n",
      "0.69 0.66 0.98\n",
      "0.71 0.65 0.98\n",
      "0.71 0.64 0.98\n",
      "\n",
      "\n",
      "cifar10+re_pruning_gd_top_k_mc_ac_dk_admm_retrain+alexnet_s\n",
      "0.1 0.82 1.0\n",
      "0.71 0.69 0.99\n",
      "0.7 0.68 0.99\n",
      "0.68 0.64 0.99\n",
      "0.69 0.63 0.98\n",
      "0.7 0.62 0.98\n",
      "0.71 0.62 0.99\n",
      "0.7 0.62 0.98\n",
      "\n",
      "\n",
      "cifar10+gd_top_k_mc_ac+resnet18\n",
      "0.75 0.37 0.0\n",
      "0.76 0.36 0.0\n",
      "0.77 0.34 0.0\n",
      "0.76 0.32 0.0\n",
      "0.76 0.31 0.0\n",
      "0.76 0.31 0.0\n",
      "\n",
      "\n",
      "cifar10+gd_top_k_mc+resnet18\n",
      "0.76 0.19 0.0\n",
      "0.76 0.17 0.0\n",
      "0.76 0.13 0.0\n",
      "0.76 0.1 0.0\n",
      "0.76 0.1 0.0\n",
      "0.77 0.09 0.0\n",
      "0.77 0.07 0.0\n",
      "\n",
      "\n",
      "cifar10+gd_top_k+resnet18\n",
      "0.76 0.19 0.0\n",
      "0.75 0.17 0.0\n",
      "0.75 0.13 0.0\n",
      "0.76 0.1 0.0\n",
      "0.76 0.09 0.0\n",
      "0.76 0.07 0.0\n",
      "\n",
      "\n",
      "cifar10+re_pruning_ac+resnet18\n",
      "0.1 0.6 0.98\n",
      "0.74 0.36 0.92\n",
      "0.74 0.36 0.89\n",
      "0.75 0.36 0.92\n",
      "\n",
      "\n",
      "cifar10+admm_intra+resnet18\n",
      "0.24 0.75 0.94\n",
      "0.64 0.73 0.94\n",
      "0.67 0.61 0.94\n",
      "0.75 0.6 0.71\n",
      "0.75 0.57 0.71\n",
      "0.72 0.49 0.71\n",
      "\n",
      "\n",
      "cifar10+gd_top_k_mc_ac_dk_admm_intra+resnet18\n",
      "0.69 0.78 0.94\n",
      "0.75 0.66 0.71\n",
      "0.75 0.66 0.71\n",
      "0.75 0.66 0.71\n",
      "0.1 0.64 0.0\n",
      "0.1 0.61 0.0\n",
      "0.1 0.59 0.0\n",
      "0.1 0.5 0.0\n",
      "0.1 0.47 0.0\n",
      "0.1 0.44 0.0\n",
      "\n",
      "\n",
      "cifar10+admm_retrain+resnet18\n",
      "0.56 0.29 0.94\n",
      "0.48 0.29 0.94\n",
      "0.68 0.24 0.71\n",
      "0.7 0.24 0.71\n",
      "0.57 0.16 0.94\n",
      "0.69 0.14 0.71\n",
      "\n",
      "\n",
      "mnist+admm_intra+lenet\n",
      "0.99 0.51 0.99\n",
      "0.99 0.5 0.99\n",
      "0.99 0.43 0.95\n",
      "0.99 0.37 0.99\n",
      "0.99 0.34 0.99\n",
      "0.99 0.34 0.73\n",
      "\n",
      "\n",
      "mnist+admm_retrain+lenet\n",
      "0.99 0.19 0.99\n",
      "0.99 0.18 0.98\n",
      "0.99 0.18 0.99\n",
      "0.99 0.18 0.99\n",
      "0.99 0.17 0.99\n",
      "0.99 0.17 0.97\n",
      "\n",
      "\n",
      "mnist+gd_top_k+lenet\n",
      "0.99 0.52 0.0\n",
      "0.99 0.51 0.0\n",
      "0.99 0.42 0.0\n",
      "0.99 0.42 0.0\n",
      "0.1 0.17 0.0\n",
      "0.1 0.17 0.0\n",
      "\n",
      "\n",
      "mnist+gd_top_k_mc+lenet\n",
      "0.99 0.42 0.0\n",
      "0.99 0.41 0.0\n",
      "0.99 0.41 0.0\n",
      "0.99 0.4 0.0\n",
      "0.1 0.11 0.0\n",
      "0.1 0.07 0.0\n",
      "\n",
      "\n",
      "mnist+gd_top_k_mc_ac+lenet\n",
      "0.99 0.57 0.0\n",
      "0.99 0.56 0.0\n",
      "0.99 0.56 0.0\n",
      "0.99 0.56 0.0\n",
      "0.99 0.56 0.0\n",
      "0.99 0.56 0.0\n",
      "0.99 0.55 0.0\n",
      "0.99 0.52 0.0\n",
      "\n",
      "\n",
      "mnist+gd_top_k_mc_ac_dk_admm_intra+lenet\n",
      "0.98 0.77 0.99\n",
      "0.99 0.71 0.94\n",
      "0.99 0.7 0.99\n",
      "0.99 0.66 0.99\n",
      "0.99 0.66 0.99\n",
      "0.98 0.65 0.99\n",
      "\n",
      "\n",
      "mnist+gd_top_k_mc_ac_dk_admm_retrain+lenet\n",
      "0.91 0.65 0.99\n",
      "0.98 0.61 0.99\n",
      "0.98 0.59 0.99\n",
      "0.99 0.57 0.99\n",
      "0.99 0.56 0.93\n",
      "0.98 0.55 0.99\n",
      "\n",
      "\n",
      "mnist+re_pruning_admm_retrain+lenet\n",
      "0.1 0.95 1.0\n",
      "0.1 0.95 1.0\n",
      "0.1 0.94 1.0\n",
      "0.1 0.94 1.0\n",
      "0.97 0.93 0.99\n",
      "0.98 0.93 0.99\n",
      "0.97 0.82 0.99\n",
      "0.97 0.82 0.99\n",
      "0.98 0.82 0.99\n",
      "0.98 0.82 0.99\n",
      "0.98 0.82 0.99\n",
      "0.98 0.82 0.99\n",
      "0.98 0.82 0.99\n",
      "0.97 0.81 0.99\n",
      "0.98 0.81 0.99\n",
      "\n",
      "\n",
      "mnist+re_pruning_admm_intra+lenet\n",
      "0.1 0.98 1.0\n",
      "0.1 0.98 1.0\n",
      "0.99 0.93 0.99\n",
      "0.99 0.93 0.99\n",
      "0.99 0.9 0.99\n",
      "0.98 0.89 0.99\n",
      "0.99 0.82 0.99\n",
      "0.99 0.81 0.99\n",
      "0.99 0.8 0.99\n",
      "0.99 0.8 0.99\n",
      "0.99 0.79 0.99\n",
      "0.99 0.79 0.99\n",
      "\n",
      "\n",
      "mnist+re_pruning_ac+lenet\n",
      "0.93 0.99 0.99\n",
      "0.97 0.98 0.99\n",
      "0.97 0.98 0.99\n",
      "0.98 0.97 0.99\n",
      "0.93 0.97 0.99\n",
      "0.98 0.96 0.99\n",
      "0.95 0.96 0.99\n",
      "0.98 0.96 0.98\n",
      "0.94 0.95 0.99\n",
      "0.98 0.95 0.99\n",
      "0.94 0.94 0.99\n",
      "0.95 0.94 0.99\n",
      "0.94 0.9 0.99\n",
      "\n",
      "\n",
      "mnist+re_pruning_ac_admm_intra+lenet\n",
      "0.1 0.98 1.0\n",
      "0.1 0.98 1.0\n",
      "0.1 0.98 1.0\n",
      "0.1 0.98 1.0\n",
      "0.1 0.98 1.0\n",
      "0.99 0.92 0.99\n",
      "0.98 0.85 0.99\n",
      "0.98 0.84 0.99\n",
      "0.98 0.84 0.99\n",
      "0.98 0.84 0.99\n",
      "0.98 0.84 0.99\n",
      "0.98 0.83 0.99\n",
      "0.98 0.83 0.99\n",
      "0.98 0.81 0.99\n",
      "0.98 0.81 0.99\n",
      "\n",
      "\n",
      "mnist+re_pruning_ac_admm_retrain+lenet\n",
      "0.1 0.98 1.0\n",
      "0.1 0.98 1.0\n",
      "0.1 0.98 1.0\n",
      "0.1 0.98 1.0\n",
      "0.98 0.94 0.99\n",
      "0.98 0.94 0.99\n",
      "0.98 0.93 0.99\n",
      "0.98 0.91 0.99\n",
      "0.99 0.9 0.99\n",
      "0.99 0.9 0.99\n",
      "0.99 0.89 0.99\n",
      "0.99 0.88 0.99\n",
      "\n",
      "\n",
      "mnist+re_pruning_gd_top_k_mc_ac_dk_admm_intra+lenet\n",
      "0.1 0.95 1.0\n",
      "0.1 0.95 1.0\n",
      "0.98 0.95 0.99\n",
      "0.99 0.94 0.99\n",
      "0.97 0.94 0.89\n",
      "0.94 0.93 0.86\n",
      "0.98 0.93 0.98\n",
      "0.98 0.91 0.99\n",
      "0.98 0.9 0.99\n",
      "0.98 0.9 0.99\n",
      "0.98 0.89 0.99\n",
      "0.98 0.87 0.92\n",
      "0.98 0.87 0.97\n",
      "0.98 0.86 0.99\n",
      "0.98 0.86 0.97\n",
      "\n",
      "\n",
      "mnist+re_pruning_gd_top_k_mc_ac_dk_admm_retrain+lenet\n",
      "0.1 1.0 1.0\n",
      "0.1 0.96 1.0\n",
      "0.97 0.93 0.99\n",
      "0.98 0.92 0.99\n",
      "0.98 0.9 0.99\n",
      "0.97 0.89 0.99\n",
      "0.98 0.83 0.99\n",
      "0.99 0.78 0.99\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_logdict = {}\n",
    "for log in logs:\n",
    "    for key in log['LOGDATA']:\n",
    "        new_key = log['METADATA']['EXPERIMENT']['dataset'] + '+'\n",
    "        new_key += log['METADATA']['EXPERIMENT']['name'] + '+'\n",
    "        new_key += log['METADATA']['EXPERIMENT']['model']\n",
    "        if new_key not in new_logdict:\n",
    "            new_logdict[new_key] = {}\n",
    "        if key not in new_logdict[new_key]:\n",
    "            new_logdict[new_key][key] = []\n",
    "        if 'overhead' in key:\n",
    "            new_logdict[new_key][key].append(sum(log['LOGDATA'][key])/len(log['LOGDATA'][key]))\n",
    "        else:\n",
    "            new_logdict[new_key][key].append(log['LOGDATA'][key][-1])\n",
    "        \n",
    "n = 15\n",
    "for key in new_logdict:\n",
    "    if not 'baseline' in key and ('+resnet' in key or '+alexnet' in key or '+lenet' in key):\n",
    "        print(key)\n",
    "        idx = sorted(range(len(new_logdict[key]['total_su'])), \n",
    "                     key=lambda i: new_logdict[key]['total_su'][i])[-n:]\n",
    "        #idx = idx[:3]\n",
    "        idx.reverse()\n",
    "        for i in idx:\n",
    "            print(round(new_logdict[key]['test_accuracy'][i],2), \n",
    "                  round(1-1/new_logdict[key]['total_su'][i],2),\n",
    "                  #round(new_logdict[key]['total_su'][i],2),\n",
    "                  round(new_logdict[key]['current_sparsity'][i],2))\n",
    "                  #round(new_logdict[key]['current_relative_overhead'][i],2))\n",
    "        print('\\n')\n",
    "    #for subkey in new_logdict[key]:\n",
    "    #    if not 'features' in subkey and not 'weight' in subkey:\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-associate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
